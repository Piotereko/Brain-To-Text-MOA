{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ffe795d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add nejm_b2txt_utils to path\n",
    "sys.path.append('../nejm_b2txt_utils')\n",
    "from general_utils import calculate_aggregate_error_rate, calculate_error_rate\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c855e6",
   "metadata": {},
   "source": [
    "## 2. Load Baseline Results\n",
    "\n",
    "Load existing baseline results from the RNN model with different language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34024d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define result paths\n",
    "baseline_path = Path('../data/t15_pretrained_rnn_baseline')\n",
    "\n",
    "# Find all detailed results files\n",
    "result_files = list(baseline_path.glob('detailed_results_*.csv'))\n",
    "print(f\"Found {len(result_files)} result files:\")\n",
    "for f in result_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6eb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest validation and test results\n",
    "def load_latest_results(split='val'):\n",
    "    pattern = baseline_path / f'detailed_results_{split}_*.csv'\n",
    "    files = list(baseline_path.glob(f'detailed_results_{split}_*.csv'))\n",
    "    if not files:\n",
    "        print(f\"No {split} results found!\")\n",
    "        return None\n",
    "    \n",
    "    latest = max(files, key=os.path.getctime)\n",
    "    print(f\"Loading {split} results: {latest.name}\")\n",
    "    df = pd.read_csv(latest)\n",
    "    return df\n",
    "\n",
    "# Load validation results\n",
    "df_val = load_latest_results('val')\n",
    "if df_val is not None:\n",
    "    print(f\"\\nValidation set: {len(df_val)} trials\")\n",
    "    print(f\"Columns: {list(df_val.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f375543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test results\n",
    "df_test = load_latest_results('test')\n",
    "if df_test is not None:\n",
    "    print(f\"\\nTest set: {len(df_test)} trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea4bf2",
   "metadata": {},
   "source": [
    "## 3. Calculate Error Rates\n",
    "\n",
    "Calculate Word Error Rate (WER) and Phoneme Error Rate (PER) for the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df):\n",
    "    \"\"\"Calculate WER and PER from detailed results.\"\"\"\n",
    "    \n",
    "    # Prepare data for WER calculation\n",
    "    true_sentences = []\n",
    "    pred_sentences = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row['true_sentence']):\n",
    "            true_sentences.append(row['true_sentence'].split())\n",
    "            pred_sentences.append(row['pred_sentence'].split() if pd.notna(row['pred_sentence']) else [])\n",
    "    \n",
    "    # Calculate aggregate WER\n",
    "    if true_sentences:\n",
    "        wer_result = calculate_aggregate_error_rate(true_sentences, pred_sentences)\n",
    "        wer_agg, wer_ci_low, wer_ci_high, wer_ind = wer_result\n",
    "    else:\n",
    "        wer_agg, wer_ci_low, wer_ci_high = None, None, None\n",
    "        wer_ind = []\n",
    "    \n",
    "    # Prepare data for PER calculation\n",
    "    true_phonemes = []\n",
    "    pred_phonemes = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row['true_phonemes']):\n",
    "            true_ph = [p.strip() for p in row['true_phonemes'].split('|') if p.strip()]\n",
    "            pred_ph = [p.strip() for p in row['pred_phonemes'].split('|') if p.strip()] if pd.notna(row['pred_phonemes']) else []\n",
    "            true_phonemes.append(true_ph)\n",
    "            pred_phonemes.append(pred_ph)\n",
    "    \n",
    "    # Calculate aggregate PER\n",
    "    if true_phonemes:\n",
    "        per_result = calculate_aggregate_error_rate(true_phonemes, pred_phonemes)\n",
    "        per_agg, per_ci_low, per_ci_high, per_ind = per_result\n",
    "    else:\n",
    "        per_agg, per_ci_low, per_ci_high = None, None, None\n",
    "        per_ind = []\n",
    "    \n",
    "    return {\n",
    "        'wer': wer_agg * 100 if wer_agg is not None else None,\n",
    "        'wer_ci_low': wer_ci_low * 100 if wer_ci_low is not None else None,\n",
    "        'wer_ci_high': wer_ci_high * 100 if wer_ci_high is not None else None,\n",
    "        'per': per_agg * 100 if per_agg is not None else None,\n",
    "        'per_ci_low': per_ci_low * 100 if per_ci_low is not None else None,\n",
    "        'per_ci_high': per_ci_high * 100 if per_ci_high is not None else None,\n",
    "        'wer_individual': wer_ind,\n",
    "        'per_individual': per_ind\n",
    "    }\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "if df_val is not None:\n",
    "    metrics_val = calculate_metrics(df_val)\n",
    "    print(\"\\n=== Baseline Validation Metrics ===\")\n",
    "    if metrics_val['wer'] is not None:\n",
    "        print(f\"WER: {metrics_val['wer']:.2f}% (95% CI: [{metrics_val['wer_ci_low']:.2f}%, {metrics_val['wer_ci_high']:.2f}%])\")\n",
    "    if metrics_val['per'] is not None:\n",
    "        print(f\"PER: {metrics_val['per']:.2f}% (95% CI: [{metrics_val['per_ci_low']:.2f}%, {metrics_val['per_ci_high']:.2f}%])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201994eb",
   "metadata": {},
   "source": [
    "## 4. Corpus-Specific Analysis\n",
    "\n",
    "Analyze performance by corpus type (Switchboard vs other corpora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_val is not None and 'corpus' in df_val.columns:\n",
    "    # Get unique corpora\n",
    "    corpora = df_val['corpus'].unique()\n",
    "    print(f\"Corpora in dataset: {corpora}\\n\")\n",
    "    \n",
    "    # Calculate metrics per corpus\n",
    "    corpus_metrics = {}\n",
    "    for corpus in corpora:\n",
    "        df_corpus = df_val[df_val['corpus'] == corpus]\n",
    "        metrics = calculate_metrics(df_corpus)\n",
    "        corpus_metrics[corpus] = metrics\n",
    "        print(f\"{corpus}: WER = {metrics['wer']:.2f}%, PER = {metrics['per']:.2f}%\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    corpora_list = list(corpus_metrics.keys())\n",
    "    wer_values = [corpus_metrics[c]['wer'] for c in corpora_list]\n",
    "    per_values = [corpus_metrics[c]['per'] for c in corpora_list]\n",
    "    \n",
    "    # WER by corpus\n",
    "    ax1.bar(corpora_list, wer_values, color='steelblue', alpha=0.7)\n",
    "    ax1.set_ylabel('Word Error Rate (%)', fontsize=12)\n",
    "    ax1.set_title('WER by Corpus', fontsize=14)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # PER by corpus\n",
    "    ax2.bar(corpora_list, per_values, color='coral', alpha=0.7)\n",
    "    ax2.set_ylabel('Phoneme Error Rate (%)', fontsize=12)\n",
    "    ax2.set_title('PER by Corpus', fontsize=14)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76258690",
   "metadata": {},
   "source": [
    "## 5. Language Model Comparison Plan\n",
    "\n",
    "To run experiments with different LMs, you'll need to:\n",
    "\n",
    "### Required LM Models\n",
    "1. **1-gram** (included): `language_model/pretrained_language_models/openwebtext_1gram_lm_sil`\n",
    "2. **3-gram** (download): From Dryad → `languageModel.tar.gz` (~60GB RAM)\n",
    "3. **5-gram** (download): From Dryad → `languageModel_5gram.tar.gz` (~300GB RAM)\n",
    "\n",
    "### Experiment Configurations\n",
    "\n",
    "| Config | LM | Neural Rescore | Alpha | Expected WER |\n",
    "|--------|----|----|-------|------|\n",
    "| baseline-1gram | 1-gram | No | 0.0 | Highest |\n",
    "| 1gram-opt | 1-gram | OPT-6.7b | 0.55 | - |\n",
    "| 3gram | 3-gram | No | 0.0 | Lower |\n",
    "| 3gram-opt | 3-gram | OPT-6.7b | 0.55 | Best |\n",
    "| 5gram | 5-gram | No | 0.0 | Lowest |\n",
    "| 5gram-opt | 5-gram | OPT-6.7b | 0.55 | Best |\n",
    "\n",
    "### Commands to Run\n",
    "\n",
    "**Terminal 1: Start Redis**\n",
    "```bash\n",
    "redis-server\n",
    "```\n",
    "\n",
    "**Terminal 2: Start LM Server** (example for 1-gram)\n",
    "```bash\n",
    "conda activate b2txt25_lm\n",
    "python language_model/language-model-standalone.py \\\n",
    "    --lm_path language_model/pretrained_language_models/openwebtext_1gram_lm_sil \\\n",
    "    --nbest 100 \\\n",
    "    --acoustic_scale 0.325 \\\n",
    "    --blank_penalty 90 \\\n",
    "    --alpha 0.0 \\\n",
    "    --redis_ip localhost \\\n",
    "    --gpu_number 0\n",
    "```\n",
    "\n",
    "**Terminal 3: Run Evaluation**\n",
    "```bash\n",
    "conda activate b2txt25\n",
    "python model_training/evaluate_model.py \\\n",
    "    --model_path ./data/t15_pretrained_rnn_baseline \\\n",
    "    --data_dir ./data/hdf5_data_final \\\n",
    "    --eval_type val \\\n",
    "    --gpu_number 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50586da3",
   "metadata": {},
   "source": [
    "## 6. Decoding Parameter Analysis\n",
    "\n",
    "Analyze the effect of key decoding parameters on rule compliance and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for parameter sensitivity analysis\n",
    "# This will be populated after running experiments with different parameters\n",
    "\n",
    "parameter_configs = {\n",
    "    'baseline': {'acoustic_scale': 0.325, 'blank_penalty': 90, 'alpha': 0.55, 'beam': 17.0},\n",
    "    'high_acoustic': {'acoustic_scale': 0.5, 'blank_penalty': 90, 'alpha': 0.55, 'beam': 17.0},\n",
    "    'low_acoustic': {'acoustic_scale': 0.1, 'blank_penalty': 90, 'alpha': 0.55, 'beam': 17.0},\n",
    "    'high_lm': {'acoustic_scale': 0.325, 'blank_penalty': 90, 'alpha': 0.8, 'beam': 17.0},\n",
    "    'low_lm': {'acoustic_scale': 0.325, 'blank_penalty': 90, 'alpha': 0.2, 'beam': 17.0},\n",
    "}\n",
    "\n",
    "print(\"Parameter configurations to test:\")\n",
    "for name, params in parameter_configs.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for k, v in params.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2e984",
   "metadata": {},
   "source": [
    "## 7. Linguistic Improvement Analysis\n",
    "\n",
    "Analyze specific types of linguistic improvements from different LMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_types(df):\n",
    "    \"\"\"Analyze types of errors made by the model.\"\"\"\n",
    "    \n",
    "    if df is None or 'true_sentence' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    error_analysis = {\n",
    "        'substitutions': [],\n",
    "        'insertions': [],\n",
    "        'deletions': [],\n",
    "        'correct': []\n",
    "    }\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row['true_sentence']) and pd.notna(row['pred_sentence']):\n",
    "            true_words = row['true_sentence'].lower().split()\n",
    "            pred_words = row['pred_sentence'].lower().split()\n",
    "            \n",
    "            # Simple analysis (can be made more sophisticated)\n",
    "            if true_words == pred_words:\n",
    "                error_analysis['correct'].append(row)\n",
    "            elif len(pred_words) > len(true_words):\n",
    "                error_analysis['insertions'].append((true_words, pred_words))\n",
    "            elif len(pred_words) < len(true_words):\n",
    "                error_analysis['deletions'].append((true_words, pred_words))\n",
    "            else:\n",
    "                error_analysis['substitutions'].append((true_words, pred_words))\n",
    "    \n",
    "    return error_analysis\n",
    "\n",
    "if df_val is not None:\n",
    "    error_analysis = analyze_error_types(df_val)\n",
    "    if error_analysis:\n",
    "        print(\"\\n=== Error Type Distribution ===\")\n",
    "        print(f\"Correct: {len(error_analysis['correct'])}\")\n",
    "        print(f\"Substitutions: {len(error_analysis['substitutions'])}\")\n",
    "        print(f\"Insertions: {len(error_analysis['insertions'])}\")\n",
    "        print(f\"Deletions: {len(error_analysis['deletions'])}\")\n",
    "        \n",
    "        # Visualize error distribution\n",
    "        error_counts = [\n",
    "            len(error_analysis['correct']),\n",
    "            len(error_analysis['substitutions']),\n",
    "            len(error_analysis['insertions']),\n",
    "            len(error_analysis['deletions'])\n",
    "        ]\n",
    "        labels = ['Correct', 'Substitutions', 'Insertions', 'Deletions']\n",
    "        colors = ['green', 'orange', 'red', 'purple']\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.pie(error_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Error Type Distribution', fontsize=14)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca505cdd",
   "metadata": {},
   "source": [
    "## 8. Example Predictions\n",
    "\n",
    "Show examples of predictions to understand model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a86a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_val is not None:\n",
    "    print(\"=== Example Predictions ===\")\n",
    "    print(\"\\nShowing 10 random examples:\\n\")\n",
    "    \n",
    "    # Sample random trials\n",
    "    sample_df = df_val[df_val['true_sentence'].notna()].sample(min(10, len(df_val)))\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"Trial {row['trial']} ({row['corpus']})\")\n",
    "        print(f\"  True:  {row['true_sentence']}\")\n",
    "        print(f\"  Pred:  {row['pred_sentence']}\")\n",
    "        \n",
    "        # Calculate WER for this trial\n",
    "        true_words = row['true_sentence'].split()\n",
    "        pred_words = row['pred_sentence'].split() if pd.notna(row['pred_sentence']) else []\n",
    "        wer = calculate_error_rate(true_words, pred_words) / len(true_words) * 100\n",
    "        print(f\"  WER:   {wer:.1f}%\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4682b7",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### Current Baseline Performance\n",
    "- **Model**: RNN + 1-gram LM (or as configured)\n",
    "- **WER**: [To be filled after running experiments]\n",
    "- **PER**: [To be filled after running experiments]\n",
    "\n",
    "### Next Experiments to Run\n",
    "1. ✓ Baseline 1-gram (already available)\n",
    "2. ⏳ 3-gram LM (download required)\n",
    "3. ⏳ 3-gram + OPT-6.7b\n",
    "4. ⏳ 5-gram LM (download required, high RAM)\n",
    "5. ⏳ Parameter tuning for rule compliance\n",
    "\n",
    "### Expected Improvements\n",
    "- **3-gram vs 1-gram**: Better grammar, reduced WER by ~5-10%\n",
    "- **Neural LM rescoring**: Better long-range context, reduced WER by ~3-5%\n",
    "- **5-gram**: Best n-gram performance, but high memory cost\n",
    "\n",
    "### Documentation to Create\n",
    "- Parameter tuning guide\n",
    "- Rule compliance validation\n",
    "- Linguistic improvement examples\n",
    "- Speed/memory trade-off analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
